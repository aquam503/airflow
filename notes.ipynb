{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you cannot use the #reader# variable outside of the with block in \n",
    "the code you provided, because it only exists within the scope of that block.\n",
    "\n",
    "The with statement is a context manager that automatically closes the file when the block \n",
    "ends. This means that the file object is only accessible within the with block, \n",
    "and any variables created within the block, such as reader, \n",
    "will also only exist within that block.\n",
    "\n",
    "If you need to use the data from the CSV file outside of the with block, you can create \n",
    "a variable outside of the block and assign the data to it within the block, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open(file_name, 'r') as file:\\n    reader = csv.DictReader(file)\\n    data = [row for row in reader]\\n\\n# Now you can use the 'data' variable outside of the 'with' block\\nprint(data)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with open(file_name, 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    data = [row for row in reader]\n",
    "\n",
    "# Now you can use the 'data' variable outside of the 'with' block\n",
    "print(data)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the data variable is assigned a list of dictionaries created \n",
    "from the CSV file data within the with block, and then it can be accessed and \n",
    "used outside of the block.\n",
    "(every dictionary represents a row in my file)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cron schedule expressions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2**** =  “At minute 2.”,The cron schedule expression 2 * * * * \n",
    "will run a job at the 2nd minute of every hour.\n",
    "\n",
    "at 2023-02-22 11:02:00\n",
    "\n",
    "then at 2023-02-22 12:02:00\n",
    "\n",
    "then at 2023-02-22 13:02:00\n",
    "\n",
    "then at 2023-02-22 14:02:00\n",
    "\n",
    "then at 2023-02-22 15:02:00\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dag = DAG(\n",
    "\n",
    "    'csv_sum_dag',\n",
    "\n",
    "    default_args=default_args,\n",
    "    \n",
    "    schedule_interval='*/2 * * * *'\n",
    ")\n",
    "\n",
    "'*/2 * * * *'= “At every 2nd minute.”\n",
    "\n",
    "at 2023-02-22 11:26:00\n",
    "\n",
    "then at 2023-02-22 11:28:00\n",
    "\n",
    "then at 2023-02-22 11:30:00\n",
    "\n",
    "then at 2023-02-22 11:32:00\n",
    "\n",
    "then at 2023-02-22 11:34:00\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a DAG in Airflow, you can trigger it manually or let Airflow handle the scheduling based on the DAG's schedule_interval setting.\n",
    "\n",
    "To trigger the DAG manually, you can use the Airflow UI or the command-line interface. Here are the steps to run a DAG manually using the Airflow UI:\n",
    "\n",
    "* 1. Open the Airflow UI in your web browser.\n",
    "* 2. Click on the DAGs link in the top menu bar to see a list of all the available DAGs.\n",
    "* 3. Find your DAG in the list and click on the On button in the far-right column to turn it on (if it's not already on).\n",
    "* 4. Click on the DAG name to go to the DAG details page.\n",
    "* 5. Click on the Trigger DAG button in the top-right corner of the page.\n",
    "\n",
    "This will start a new DAG run immediately, and you can monitor its progress in the Airflow UI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open the Airflow UI in your web browser."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open the Airflow UI in your web browser, you'll need to have Airflow installed and running on your computer or server. Here are the general steps to do that:\n",
    "\n",
    "Install Airflow using a package manager like pip, conda, or apt-get, depending on your system.\n",
    "Configure Airflow by setting up a configuration file (usually called airflow.cfg) with your desired settings, including database connection information and other options.\n",
    "Initialize the Airflow database by running the airflow db init command, which creates the necessary database tables and structures.\n",
    "Start the Airflow scheduler and webserver processes by running the airflow scheduler and airflow webserver commands in separate terminal windows or processes.\n",
    "Once Airflow is running, you can open the Airflow UI in your web browser by navigating to the URL for the Airflow webserver. By default, this is usually http://localhost:8080 or http://127.0.0.1:8080, but it may be different depending on your Airflow configuration. Check your airflow.cfg file or the output of the airflow webserver command for the exact URL to use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The airflow db init command is used to initialize the Airflow metadata database, which is used to store all of the configuration, status, and history information for your Airflow workflows.\n",
    "\n",
    "When you run this command, Airflow will create a new database (or update an existing one) using the database settings specified in your Airflow configuration file (airflow.cfg). This database will contain all of the tables and schema needed to store information about your DAGs, tasks, task instances, logs, and more.\n",
    "\n",
    "Running airflow db init is usually one of the first steps you'll need to do when setting up a new Airflow installation or when making significant changes to your Airflow configuration. It ensures that your Airflow metadata database is properly configured and up to date with the latest schema changes.\n",
    "\n",
    "Note that running airflow db init will not delete or modify any existing data in your Airflow metadata database. If you need to update the schema or make other changes to the database, you may need to use other database migration tools or commands provided by your specific database management system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"webserver.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"scheduler.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'a':1}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
